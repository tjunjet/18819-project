{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54754da0-f94d-4468-81a9-1c61607c217a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 18-819 QML Project running QVC to QiSkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe363d14-b88d-4348-a9cc-a72ebf41d72d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Braket SDK Cost Tracking to estimate cost\n",
    "from braket.tracking import Tracker\n",
    "t = Tracker().start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57decc-4d1b-48f9-8fa9-9e7db5a44484",
   "metadata": {},
   "source": [
    "### Accessing Quantum Processing Units\n",
    "\n",
    "We will access third party QPUs to run our training on quantum hardware. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0427cec-bb47-4e71-97fe-bc0d34d47d98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BraketBackend[Aria 1], BraketBackend[Forte 1], BraketBackend[Garnet]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qiskit_braket_provider import BraketProvider\n",
    "\n",
    "provider = BraketProvider()\n",
    "\n",
    "# Seeing which QPUs are online\n",
    "provider.backends(statuses=[\"ONLINE\"], types=[\"QPU\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e860b183-caa1-437a-bcd3-805760469176",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BraketBackend[Aria 1]\n"
     ]
    }
   ],
   "source": [
    "qpu_backend = provider.get_backend(\"Aria 1\")\n",
    "\n",
    "print(qpu_backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d73d7-9c45-4ef2-b5c5-b4e421c2c711",
   "metadata": {},
   "source": [
    "### Loading the Exoplanet training data\n",
    "\n",
    "The exoplanet training data has already been separated into X_train, X_test, y_train, y_test. \n",
    "\n",
    "We will use it for the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc2c844-9be5-4d9b-b749-92b9bad1f796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0f28c4-9662-4bdd-9a88-8fec31dfc729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2000, 37)\n",
      "X_test shape: (2000, 37)\n",
      "Y_train shape: (2000,)\n",
      "Y_test shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.loadtxt('middle_data/X_train.txt', delimiter=' ')\n",
    "X_test  = np.loadtxt('middle_data/X_test.txt', delimiter=' ')\n",
    "\n",
    "Y_train = np.loadtxt('middle_data/Y_train.txt', delimiter=' ')\n",
    "Y_test  = np.loadtxt('middle_data/Y_test.txt', delimiter=' ')\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc664f9-038b-45a1-b7c6-50316e209545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375912a7-2de5-4e51-a259-0f6daf233621",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Data pre-processing \n",
    "\n",
    "In this step, we will perform normalization of the X dataset, and label y in binary terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c36d7599-1004-4ccd-bf25-c75b3aa0afaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/Braket/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: pylatexenc in /home/ec2-user/anaconda3/envs/Braket/lib/python3.10/site-packages (2.10)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ec2-user/anaconda3/envs/Braket/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/Braket/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/Braket/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/Braket/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!/home/ec2-user/anaconda3/envs/Braket/bin/python -m pip install scikit-learn pylatexenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433f5f43-456a-4fc0-8975-15a1d9f14ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after selection: (2000, 2)\n",
      "X_test shape after selection: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# Normalize data to the range [-1, 1] (use the training set to fit the scaler)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ensure labels are binary (if not already binary)\n",
    "Y_train = (Y_train > np.median(Y_train)).astype(int)\n",
    "Y_test = (Y_test > np.median(Y_test)).astype(int)\n",
    "\n",
    "print(f\"X_train shape after selection: {X_train.shape}\")\n",
    "print(f\"X_test shape after selection: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992bf1c-41e1-4f53-b20e-7620573e67a8",
   "metadata": {},
   "source": [
    "Encoding the data into quantum states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920cd60-b70b-41ef-9789-bdb610803ad7",
   "metadata": {},
   "source": [
    "### 2. Defining the ZZFeatureMap\n",
    "\n",
    "The ZZFeatureMap encodes the classical data into a quantum state. The depth determines the number of repeated layers in the feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4eb1626-aeb9-47e7-8da1-9513da6d0aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1, entanglement='linear')\n",
    "# feature_map.decompose().draw(output=\"mpl\", style=\"clifford\", fold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b351f69-b77e-4648-a641-5c40ee583269",
   "metadata": {},
   "source": [
    "### 3. Define a variational Ansatz\n",
    "\n",
    "The variational ansatz applies trainable gates to the quantum state produced by the feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "591f6bd6-8138-4a48-9d14-a82103b1a77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import RealAmplitudes\n",
    "\n",
    "ansatz = RealAmplitudes(num_qubits=num_features, reps=2, entanglement='linear')\n",
    "# ansatz.decompose().draw(output=\"mpl\", style=\"clifford\", fold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351d016-b46d-46b6-b4a0-0dd025fd948a",
   "metadata": {},
   "source": [
    "### 4. Define the Model: Variational Quantum Classifier\n",
    "\n",
    "Use the VQC (Variational Quantum Classifier) from Qiskit's machine learning module.\n",
    "\n",
    "Documentation: https://qiskit-community.github.io/qiskit-machine-learning/_modules/qiskit_machine_learning/algorithms/classifiers/vqc.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb4a338-a54f-4052-9bc5-cb044e71c67c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StatevectorSampler.__init__() got an unexpected keyword argument 'backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m COBYLA(maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the Sampler primitive\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[43mStatevectorSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqpu_backend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Defining a callback to see the iteration\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverbose_callback\u001b[39m(weights, objective_value):\n",
      "\u001b[0;31mTypeError\u001b[0m: StatevectorSampler.__init__() got an unexpected keyword argument 'backend'"
     ]
    }
   ],
   "source": [
    "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit.primitives import StatevectorSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = COBYLA(maxiter=100)\n",
    "\n",
    "# Initialize the Sampler primitive\n",
    "sampler = StatevectorSampler(backend=qpu_backend)\n",
    "\n",
    "# Defining a callback to see the iteration\n",
    "def verbose_callback(weights, objective_value):\n",
    "    \"\"\"\n",
    "    Verbose callback function to monitor the training process during fit.\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): Current variational circuit parameters.\n",
    "        objective_value (float): Current loss value.\n",
    "    \"\"\"\n",
    "    print(f\"Iteration: Objective Value = {objective_value:.4f}, Weights = {weights}\")\n",
    "\n",
    "# Initialize a global progress bar\n",
    "progress_bar = tqdm(total=100, desc=\"Training Progress\")  # Replace 100 with your optimizer's max iterations\n",
    "\n",
    "def tqdm_callback(weights, objective_value):\n",
    "    \"\"\"\n",
    "    Callback function to update the tqdm progress bar during training.\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): Current variational circuit parameters.\n",
    "        objective_value (float): Current value of the loss function.\n",
    "    \"\"\"\n",
    "    progress_bar.update(1)\n",
    "    progress_bar.set_postfix({\"Loss\": objective_value})\n",
    "\n",
    "# Build the Quantum Variational Classifier\n",
    "# Num Qubits is specified as None, so that we just use the number of qubits\n",
    "# specified by the quantum feature map\n",
    "\n",
    "vqc = VQC(feature_map=feature_map, \n",
    "          ansatz=ansatz, \n",
    "          optimizer=optimizer,\n",
    "          sampler=sampler,\n",
    "          callback=verbose_callback\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375527a8-4d5b-42eb-8d7b-e5a126ada9cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Train the VQC\n",
    "\n",
    "Now we will start training the quantum circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ccf2a9-bef9-463c-a658-eca00c6c7f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Progress:   1%|          | 1/100 [00:16<26:44, 16.21s/it]\u001b[A\n",
      "Training Progress:   1%|          | 1/100 [00:16<26:44, 16.21s/it, Loss=1.03]\u001b[A\n",
      "Training Progress:   2%|▏         | 2/100 [00:29<23:12, 14.21s/it, Loss=1.03]\u001b[A\n",
      "Training Progress:   2%|▏         | 2/100 [00:29<23:12, 14.21s/it, Loss=0.988]\u001b[A\n",
      "Training Progress:   3%|▎         | 3/100 [00:44<23:35, 14.59s/it, Loss=0.988]\u001b[A\n",
      "Training Progress:   3%|▎         | 3/100 [00:44<23:35, 14.59s/it, Loss=0.947]\u001b[A\n",
      "Training Progress:   4%|▍         | 4/100 [01:00<24:16, 15.17s/it, Loss=0.947]\u001b[A\n",
      "Training Progress:   4%|▍         | 4/100 [01:00<24:16, 15.17s/it, Loss=1.04] \u001b[A\n",
      "Training Progress:   5%|▌         | 5/100 [01:15<24:08, 15.25s/it, Loss=1.04]\u001b[A\n",
      "Training Progress:   5%|▌         | 5/100 [01:15<24:08, 15.25s/it, Loss=0.962]\u001b[A\n",
      "Training Progress:   6%|▌         | 6/100 [01:28<22:55, 14.63s/it, Loss=0.962]\u001b[A\n",
      "Training Progress:   6%|▌         | 6/100 [01:28<22:55, 14.63s/it, Loss=1.25] \u001b[A\n",
      "Training Progress:   7%|▋         | 7/100 [01:45<23:29, 15.16s/it, Loss=1.25]\u001b[A\n",
      "Training Progress:   7%|▋         | 7/100 [01:45<23:29, 15.16s/it, Loss=0.947]\u001b[A\n",
      "Training Progress:   8%|▊         | 8/100 [02:00<23:17, 15.19s/it, Loss=0.947]\u001b[A\n",
      "Training Progress:   8%|▊         | 8/100 [02:00<23:17, 15.19s/it, Loss=1.12] \u001b[A\n",
      "Training Progress:   9%|▉         | 9/100 [02:17<23:47, 15.69s/it, Loss=1.12]\u001b[A\n",
      "Training Progress:   9%|▉         | 9/100 [02:17<23:47, 15.69s/it, Loss=0.969]\u001b[A\n",
      "Training Progress:  10%|█         | 10/100 [02:32<23:25, 15.62s/it, Loss=0.969]\u001b[A\n",
      "Training Progress:  10%|█         | 10/100 [02:32<23:25, 15.62s/it, Loss=0.942]\u001b[A\n",
      "Training Progress:  11%|█         | 11/100 [02:47<22:39, 15.28s/it, Loss=0.942]\u001b[A\n",
      "Training Progress:  11%|█         | 11/100 [02:47<22:39, 15.28s/it, Loss=0.971]\u001b[A\n",
      "Training Progress:  12%|█▏        | 12/100 [03:02<22:19, 15.22s/it, Loss=0.971]\u001b[A\n",
      "Training Progress:  12%|█▏        | 12/100 [03:02<22:19, 15.22s/it, Loss=0.946]\u001b[A\n",
      "Training Progress:  13%|█▎        | 13/100 [03:17<22:06, 15.25s/it, Loss=0.946]\u001b[A\n",
      "Training Progress:  13%|█▎        | 13/100 [03:17<22:06, 15.25s/it, Loss=1]    \u001b[A\n",
      "Training Progress:  14%|█▍        | 14/100 [03:31<21:27, 14.97s/it, Loss=1]\u001b[A\n",
      "Training Progress:  14%|█▍        | 14/100 [03:31<21:27, 14.97s/it, Loss=0.946]\u001b[A\n",
      "Training Progress:  15%|█▌        | 15/100 [03:45<20:48, 14.69s/it, Loss=0.946]\u001b[A\n",
      "Training Progress:  15%|█▌        | 15/100 [03:45<20:48, 14.69s/it, Loss=0.944]\u001b[A\n",
      "Training Progress:  16%|█▌        | 16/100 [03:59<19:59, 14.28s/it, Loss=0.944]\u001b[A\n",
      "Training Progress:  16%|█▌        | 16/100 [03:59<19:59, 14.28s/it, Loss=0.947]\u001b[A\n",
      "Training Progress:  17%|█▋        | 17/100 [04:12<19:14, 13.91s/it, Loss=0.947]\u001b[A\n",
      "Training Progress:  17%|█▋        | 17/100 [04:12<19:14, 13.91s/it, Loss=0.947]\u001b[A\n",
      "Training Progress:  18%|█▊        | 18/100 [04:26<19:06, 13.98s/it, Loss=0.947]\u001b[A\n",
      "Training Progress:  18%|█▊        | 18/100 [04:26<19:06, 13.98s/it, Loss=0.943]\u001b[A\n",
      "Training Progress:  19%|█▉        | 19/100 [04:41<19:16, 14.28s/it, Loss=0.943]\u001b[A\n",
      "Training Progress:  19%|█▉        | 19/100 [04:41<19:16, 14.28s/it, Loss=0.942]\u001b[A\n",
      "Training Progress:  20%|██        | 20/100 [04:55<18:54, 14.18s/it, Loss=0.942]\u001b[A\n",
      "Training Progress:  20%|██        | 20/100 [04:55<18:54, 14.18s/it, Loss=0.94] \u001b[A\n",
      "Training Progress:  21%|██        | 21/100 [05:11<19:17, 14.65s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  21%|██        | 21/100 [05:11<19:17, 14.65s/it, Loss=0.941]\u001b[A\n",
      "Training Progress:  22%|██▏       | 22/100 [05:24<18:38, 14.35s/it, Loss=0.941]\u001b[A\n",
      "Training Progress:  22%|██▏       | 22/100 [05:24<18:38, 14.35s/it, Loss=0.941]\u001b[A\n",
      "Training Progress:  23%|██▎       | 23/100 [05:40<18:52, 14.70s/it, Loss=0.941]\u001b[A\n",
      "Training Progress:  23%|██▎       | 23/100 [05:40<18:52, 14.70s/it, Loss=0.943]\u001b[A\n",
      "Training Progress:  24%|██▍       | 24/100 [05:53<18:13, 14.38s/it, Loss=0.943]\u001b[A\n",
      "Training Progress:  24%|██▍       | 24/100 [05:53<18:13, 14.38s/it, Loss=0.943]\u001b[A\n",
      "Training Progress:  25%|██▌       | 25/100 [06:08<18:13, 14.58s/it, Loss=0.943]\u001b[A\n",
      "Training Progress:  25%|██▌       | 25/100 [06:08<18:13, 14.58s/it, Loss=0.94] \u001b[A\n",
      "Training Progress:  26%|██▌       | 26/100 [06:23<17:49, 14.45s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  26%|██▌       | 26/100 [06:23<17:49, 14.45s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  27%|██▋       | 27/100 [06:38<17:47, 14.62s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  27%|██▋       | 27/100 [06:38<17:47, 14.62s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  28%|██▊       | 28/100 [06:53<17:39, 14.72s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  28%|██▊       | 28/100 [06:53<17:39, 14.72s/it, Loss=0.94] \u001b[A\n",
      "Training Progress:  29%|██▉       | 29/100 [07:07<17:17, 14.61s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  29%|██▉       | 29/100 [07:07<17:17, 14.61s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  30%|███       | 30/100 [07:22<17:12, 14.76s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  30%|███       | 30/100 [07:22<17:12, 14.76s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  31%|███       | 31/100 [07:36<16:47, 14.60s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  31%|███       | 31/100 [07:36<16:47, 14.60s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  32%|███▏      | 32/100 [07:51<16:41, 14.72s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  32%|███▏      | 32/100 [07:51<16:41, 14.72s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  33%|███▎      | 33/100 [08:08<17:01, 15.25s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  33%|███▎      | 33/100 [08:08<17:01, 15.25s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  34%|███▍      | 34/100 [08:21<16:11, 14.72s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  34%|███▍      | 34/100 [08:21<16:11, 14.72s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  35%|███▌      | 35/100 [08:36<15:50, 14.62s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  35%|███▌      | 35/100 [08:36<15:50, 14.62s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  36%|███▌      | 36/100 [08:51<15:50, 14.85s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  36%|███▌      | 36/100 [08:51<15:50, 14.85s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  37%|███▋      | 37/100 [09:07<15:52, 15.12s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  37%|███▋      | 37/100 [09:07<15:52, 15.12s/it, Loss=0.94] \u001b[A\n",
      "Training Progress:  38%|███▊      | 38/100 [09:22<15:38, 15.14s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  38%|███▊      | 38/100 [09:22<15:38, 15.14s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  39%|███▉      | 39/100 [09:37<15:18, 15.06s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  39%|███▉      | 39/100 [09:37<15:18, 15.06s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  40%|████      | 40/100 [09:52<14:59, 14.99s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  40%|████      | 40/100 [09:52<14:59, 14.99s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  41%|████      | 41/100 [10:07<14:44, 15.00s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  41%|████      | 41/100 [10:07<14:44, 15.00s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  42%|████▏     | 42/100 [10:20<14:03, 14.54s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  42%|████▏     | 42/100 [10:20<14:03, 14.54s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  43%|████▎     | 43/100 [10:35<13:58, 14.72s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  43%|████▎     | 43/100 [10:35<13:58, 14.72s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  44%|████▍     | 44/100 [10:50<13:47, 14.78s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  44%|████▍     | 44/100 [10:50<13:47, 14.78s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  45%|████▌     | 45/100 [11:06<13:46, 15.02s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  45%|████▌     | 45/100 [11:06<13:46, 15.02s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  46%|████▌     | 46/100 [11:22<13:44, 15.27s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  46%|████▌     | 46/100 [11:22<13:44, 15.27s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  47%|████▋     | 47/100 [11:37<13:32, 15.33s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  47%|████▋     | 47/100 [11:37<13:32, 15.33s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  48%|████▊     | 48/100 [11:52<13:13, 15.26s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  48%|████▊     | 48/100 [11:52<13:13, 15.26s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  49%|████▉     | 49/100 [12:08<13:08, 15.47s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  49%|████▉     | 49/100 [12:08<13:08, 15.47s/it, Loss=0.94] \u001b[A\n",
      "Training Progress:  50%|█████     | 50/100 [12:22<12:33, 15.07s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  50%|█████     | 50/100 [12:22<12:33, 15.07s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  51%|█████     | 51/100 [12:37<12:18, 15.07s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  51%|█████     | 51/100 [12:37<12:18, 15.07s/it, Loss=0.94] \u001b[A\n",
      "Training Progress:  52%|█████▏    | 52/100 [12:53<12:17, 15.37s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  52%|█████▏    | 52/100 [12:53<12:17, 15.37s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  53%|█████▎    | 53/100 [13:09<12:04, 15.42s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  53%|█████▎    | 53/100 [13:09<12:04, 15.42s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  54%|█████▍    | 54/100 [13:24<11:43, 15.30s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  54%|█████▍    | 54/100 [13:24<11:43, 15.30s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  55%|█████▌    | 55/100 [13:40<11:38, 15.52s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  55%|█████▌    | 55/100 [13:40<11:38, 15.52s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  56%|█████▌    | 56/100 [13:54<10:55, 14.90s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  56%|█████▌    | 56/100 [13:54<10:55, 14.90s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  57%|█████▋    | 57/100 [14:10<10:58, 15.30s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  57%|█████▋    | 57/100 [14:10<10:58, 15.30s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  58%|█████▊    | 58/100 [14:25<10:43, 15.33s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  58%|█████▊    | 58/100 [14:25<10:43, 15.33s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  59%|█████▉    | 59/100 [14:41<10:35, 15.49s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  59%|█████▉    | 59/100 [14:41<10:35, 15.49s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  60%|██████    | 60/100 [14:56<10:15, 15.38s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  60%|██████    | 60/100 [14:56<10:15, 15.38s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  61%|██████    | 61/100 [15:11<09:57, 15.33s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  61%|██████    | 61/100 [15:11<09:57, 15.33s/it, Loss=0.935]\u001b[A\n",
      "Training Progress:  62%|██████▏   | 62/100 [15:26<09:32, 15.08s/it, Loss=0.935]\u001b[A\n",
      "Training Progress:  62%|██████▏   | 62/100 [15:26<09:32, 15.08s/it, Loss=0.94] \u001b[A\n",
      "Training Progress:  63%|██████▎   | 63/100 [15:40<09:10, 14.88s/it, Loss=0.94]\u001b[A\n",
      "Training Progress:  63%|██████▎   | 63/100 [15:40<09:10, 14.88s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  64%|██████▍   | 64/100 [15:55<08:53, 14.82s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  64%|██████▍   | 64/100 [15:55<08:53, 14.82s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  65%|██████▌   | 65/100 [16:10<08:38, 14.82s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  65%|██████▌   | 65/100 [16:10<08:38, 14.82s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  66%|██████▌   | 66/100 [16:24<08:20, 14.71s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  66%|██████▌   | 66/100 [16:24<08:20, 14.71s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  67%|██████▋   | 67/100 [16:40<08:13, 14.95s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  67%|██████▋   | 67/100 [16:40<08:13, 14.95s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  68%|██████▊   | 68/100 [16:54<07:53, 14.81s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  68%|██████▊   | 68/100 [16:54<07:53, 14.81s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  69%|██████▉   | 69/100 [17:11<07:59, 15.47s/it, Loss=0.937]\u001b[A\n",
      "Training Progress:  69%|██████▉   | 69/100 [17:11<07:59, 15.47s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  70%|███████   | 70/100 [17:27<07:49, 15.66s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  70%|███████   | 70/100 [17:27<07:49, 15.66s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  71%|███████   | 71/100 [17:42<07:26, 15.38s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  71%|███████   | 71/100 [17:42<07:26, 15.38s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  72%|███████▏  | 72/100 [17:57<07:04, 15.16s/it, Loss=0.938]\u001b[A\n",
      "Training Progress:  72%|███████▏  | 72/100 [17:57<07:04, 15.16s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  73%|███████▎  | 73/100 [18:11<06:46, 15.05s/it, Loss=0.939]\u001b[A\n",
      "Training Progress:  73%|███████▎  | 73/100 [18:11<06:46, 15.05s/it, Loss=0.938]\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<qiskit_machine_learning.algorithms.classifiers.vqc.VQC at 0x7fc697cb7c70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqc.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072dd6c-28c9-4b6a-9dbb-545ca1480285",
   "metadata": {},
   "source": [
    "### 6. Testing of the VQC\n",
    "\n",
    "Here, we will figure out the accuracy of the VQC, both on the train set and the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2f77d-dbf1-4136-8e7c-d45c9b75122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_q4 = vqc.score(X_train, Y_train)\n",
    "test_score_q4 = vqc.score(X_test, Y_test)\n",
    "\n",
    "print(f\"Quantum VQC on the training dataset: {train_score_q4:.2f}\")\n",
    "print(f\"Quantum VQC on the test dataset:     {test_score_q4:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf2379-9579-4c23-823c-076bda28a76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
